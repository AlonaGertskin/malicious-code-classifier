def negative_binomial_objective(y_true, y_pred):
    p = 0.5

    def loss(x, t):
        loss = (
            gamma(x) + gamma(t + 1) - gamma(x + t) - x * np.log(p) - t * np.log(1 - p)
        )
        return loss

    def compute_grad(x):
        return derivative(f=lambda x: loss(x, y_true), x=x)

    grad = compute_grad(x=y_pred)
    hess = derivative(f=compute_grad, x=y_pred)  # second derivative

    return grad, hess
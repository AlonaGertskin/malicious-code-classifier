#!/usr/bin/env python3
import argparse
import json
import os
import re
import sys
from dataclasses import dataclass
from typing import Dict, Iterable, List, Optional, Set, Tuple

import urllib.request
import urllib.error


# --- Regexes (inspired by PortSwigger's JS Miner DependencyConfusion)

DEPENDENCIES_REGEX = re.compile(
	"\"(devDependencies|dependencies|peerDependencies|optionalDependencies)\"\s*:\s*\{(.*?)\}",
	re.IGNORECASE | re.DOTALL,
)

NODE_MODULES_PATH_REGEX = re.compile(r"/node_modules/(@[^/]+/[^/\s'\"\\]+|[^/\s'\"\\]+)")

SCOPED_ORG_REGEX = re.compile(r"^@([^/]+)/")


@dataclass
class ScanConfig:
	include_regex_scan: bool = False
	scannable_extensions: Set[str] = None
	excluded_directories: Set[str] = None
	timeout_seconds: float = 8.0
	enable_network_checks: bool = True


@dataclass
class DependencyFinding:
	title: str
	detail: str
	severity: str
	package_name: Optional[str] = None
	package_version: Optional[str] = None
	location: Optional[str] = None

	def to_dict(self) -> Dict[str, Optional[str]]:
		return {
			"title": self.title,
			"detail": self.detail,
			"severity": self.severity,
			"package_name": self.package_name,
			"package_version": self.package_version,
			"location": self.location,
		}


class DependencyScanner:
	def __init__(self, config: ScanConfig) -> None:
		self.config = config

	def scan_path(self, root_path: str) -> List[DependencyFinding]:
		findings: List[DependencyFinding] = []
		packages = self._collect_packages(root_path)

		if packages:
			joined = ", ".join(sorted(f"{name}@{ver}" if ver else name for name, ver, _ in packages))
			findings.append(
				DependencyFinding(
					title="Dependencies found",
					detail=f"The following dependencies were found: {joined}",
					severity="info",
				)
			)

		for package_name, package_version, location in packages:
			findings.extend(self._analyze_package(package_name, package_version, location))

		return findings

	def scan_urls(self, urls: List[str]) -> List[DependencyFinding]:
		findings: List[DependencyFinding] = []
		packages: List[Tuple[str, Optional[str], Optional[str]]] = []
		for url in urls:
			text = self._http_get_text(url)
			if not text:
				continue
			packages.extend(self._extract_from_content_regex(text, url))

		packages = self._dedupe_packages(packages)

		if packages:
			joined = ", ".join(sorted(f"{name}@{ver}" if ver else name for name, ver, _ in packages))
			findings.append(
				DependencyFinding(
					title="Dependencies found",
					detail=f"The following dependencies were found: {joined}",
					severity="info",
				)
			)

		for package_name, package_version, location in packages:
			findings.extend(self._analyze_package(package_name, package_version, location))

		return findings

	def _collect_packages(self, root_path: str) -> List[Tuple[str, Optional[str], Optional[str]]]:
		packages: List[Tuple[str, Optional[str], Optional[str]]] = []

		for dirpath, dirnames, filenames in os.walk(root_path):
			dirnames[:] = [d for d in dirnames if d not in (self.config.excluded_directories or set())]

			if "package.json" in filenames:
				pkg_json_path = os.path.join(dirpath, "package.json")
				packages.extend(self._extract_from_package_json(pkg_json_path))

			if self.config.include_regex_scan:
				for filename in filenames:
					_, ext = os.path.splitext(filename)
					if self.config.scannable_extensions and ext not in self.config.scannable_extensions:
						continue
					file_path = os.path.join(dirpath, filename)
					packages.extend(self._extract_from_file_regex(file_path))

		return self._dedupe_packages(packages)

	def _extract_from_package_json(self, path: str) -> List[Tuple[str, Optional[str], str]]:
		results: List[Tuple[str, Optional[str], str]] = []
		try:
			with open(path, "r", encoding="utf-8", errors="ignore") as f:
				data = json.load(f)
			for section in [
				"dependencies",
				"devDependencies",
				"peerDependencies",
				"optionalDependencies",
			]:
				deps = data.get(section) or {}
				for name, version in deps.items():
					results.append((name, str(version), path))
		except Exception:
			pass
		return results

	def _extract_from_file_regex(self, path: str) -> List[Tuple[str, Optional[str], str]]:
		results: List[Tuple[str, Optional[str], str]] = []
		try:
			with open(path, "r", encoding="utf-8", errors="ignore") as f:
				content = f.read()
			results.extend(self._extract_from_content_regex(content, path))
		except Exception:
			pass
		return results

	def _extract_from_content_regex(self, content: str, location: str) -> List[Tuple[str, Optional[str], str]]:
		results: List[Tuple[str, Optional[str], str]] = []
		normalized = content.replace("\t", "").replace("\r", "").replace("\n", "")
		for match in DEPENDENCIES_REGEX.finditer(normalized):
			inner = match.group(2)
			for token in inner.split(","):
				parts = token.split(":")
				if len(parts) >= 2:
					name = parts[0].strip().strip('\"\'')
					version = parts[1].strip().strip('\"\'')
					if name:
						results.append((name, version, location))
		for m in NODE_MODULES_PATH_REGEX.finditer(content):
			name = m.group(1)
			if name:
				results.append((name, None, location))
		return results

	def _dedupe_packages(self, items: List[Tuple[str, Optional[str], Optional[str]]]) -> List[Tuple[str, Optional[str], Optional[str]]]:
		seen: Set[Tuple[str, Optional[str]]] = set()
		result: List[Tuple[str, Optional[str], Optional[str]]] = []
		for name, version, location in items:
			key = (name, version)
			if key in seen:
				continue
			seen.add(key)
			result.append((name, version, location))
		return result

	def _analyze_package(self, name: str, version: Optional[str], location: Optional[str]) -> List[DependencyFinding]:
		findings: List[DependencyFinding] = []

		if version and self._looks_like_non_registry_version(version):
			findings.append(
				DependencyFinding(
					title="Dependency uses non-registry version",
					detail="Version string is not a plain semver; may reference private registry, git, or file path.",
					severity="info",
					package_name=name,
					package_version=version,
					location=location,
				)
			)

		org = self._extract_org_from_scoped(name)
		if org and self.config.enable_network_checks:
			status = self._http_status(f"https://www.npmjs.com/org/{org}")
			if status == 404:
				findings.append(
					DependencyFinding(
						title="Scoped package organization not found",
						detail=f"Organization '@{org}' does not exist on npmjs.com; may be claimable.",
						severity="high",
						package_name=name,
						package_version=version,
						location=location,
					)
				)

		if self.config.enable_network_checks:
			status = self._http_status(f"https://registry.npmjs.org/{name}")
			if status == 404:
				findings.append(
					DependencyFinding(
						title="Package not found on npm registry",
						detail="No entry found on registry.npmjs.org; potential dependency confusion risk.",
						severity="high",
						package_name=name,
						package_version=version,
						location=location,
					)
				)

		return findings

	def _looks_like_non_registry_version(self, version: str) -> bool:
		version_lower = version.strip().lower()
		if any(prefix in version_lower for prefix in ["git+", "http://", "https://", "ssh://", "file:", "link:", "workspace:"]):
			return True
		if version_lower.endswith(".tgz") or ":" in version_lower and "/" in version_lower:
			return True
		semver_like = re.compile(r"^[vV]?\d+\.\d+\.\d+([-+].*)?$|^[~^><=*xX].+")
		return not bool(semver_like.match(version_lower))

	def _extract_org_from_scoped(self, name: str) -> Optional[str]:
		m = SCOPED_ORG_REGEX.match(name)
		return m.group(1) if m else None

	def _http_status(self, url: str) -> Optional[int]:
		try:
			req = urllib.request.Request(url, headers={"User-Agent": "depconf-scanner/0.1"})
			with urllib.request.urlopen(req, timeout=self.config.timeout_seconds) as resp:
				return resp.getcode()
		except urllib.error.HTTPError as e:
			return e.code
		except Exception:
			return None

	def _http_get_text(self, url: str) -> Optional[str]:
		try:
			req = urllib.request.Request(url, headers={"User-Agent": "depconf-scanner/0.1"})
			with urllib.request.urlopen(req, timeout=self.config.timeout_seconds) as resp:
				ct = resp.headers.get("Content-Type", "")
				charset = "utf-8"
				m = re.search(r"charset=([^;]+)", ct)
				if m:
					charset = m.group(1).strip()
				return resp.read().decode(charset, errors="ignore")
		except Exception:
			return None


def parse_args(argv: Optional[List[str]] = None) -> argparse.Namespace:
	parser = argparse.ArgumentParser(
		description="Scan a project for potential dependency confusion risks (npm).",
	)
	parser.add_argument(
		"path",
		nargs="?",
		default=".",
		help="Path to project root to scan (default: current directory)",
	)
	parser.add_argument(
		"--include-regex",
		action="store_true",
		help="Also scan source files for minified dependency blocks and /node_modules/ paths",
	)
	parser.add_argument(
		"--extensions",
		default=".js,.jsx,.ts,.tsx,.mjs,.cjs,.html,.htm,.json",
		help="Comma-separated list of file extensions to scan when --include-regex is set",
	)
	parser.add_argument(
		"--exclude-dirs",
		default=".git,node_modules,dist,build,.next,.cache,.venv,venv",
		help="Comma-separated list of directories to skip",
	)
	parser.add_argument(
		"--json",
		action="store_true",
		help="Output findings as JSON",
	)
	parser.add_argument(
		"--urls-file",
		default=None,
		help="Path to a file containing JavaScript URLs to fetch and analyze (one per line)",
	)
	parser.add_argument(
		"--timeout",
		type=float,
		default=8.0,
		help="HTTP timeout for registry checks in seconds (default: 8)",
	)
	parser.add_argument(
		"--no-network",
		action="store_true",
		help="Do not query npm registry (offline mode)",
	)
	return parser.parse_args(argv)


def main(argv: Optional[List[str]] = None) -> int:
	args = parse_args(argv)
	root_path = os.path.abspath(args.path)

	config = ScanConfig(
		include_regex_scan=args.include_regex,
		scannable_extensions=set(e.strip() for e in args.extensions.split(",") if e.strip()),
		excluded_directories=set(d.strip() for d in args.exclude_dirs.split(",") if d.strip()),
		timeout_seconds=args.timeout,
		enable_network_checks=not args.no_network,
	)

	scanner = DependencyScanner(config)
	findings = scanner.scan_path(root_path)

	if args.urls_file:
		if args.no_network:
			print("Error: --urls-file requires network access. Do not use with --no-network.", file=sys.stderr)
			return 2
		try:
			with open(os.path.abspath(args.urls_file), "r", encoding="utf-8", errors="ignore") as f:
				urls = [line.strip() for line in f if line.strip() and not line.strip().startswith("#")]
		except Exception as e:
			print(f"Error reading --urls-file: {e}", file=sys.stderr)
			return 2
		findings.extend(scanner.scan_urls(urls))

	if args.json:
		print(json.dumps([f.to_dict() for f in findings], indent=2))
	else:
		if not findings:
			print("No potential dependency confusion issues found.")
		else:
			for f in findings:
				print(f"[{f.severity}] {f.title}")
				print(f"  details: {f.detail}")
				if f.package_name:
					print(f"  package: {f.package_name}")
				if f.package_version:
					print(f"  version: {f.package_version}")
				if f.location:
					print(f"  location: {f.location}")
				print()

	return 0


if __name__ == "__main__":
	sys.exit(main())

